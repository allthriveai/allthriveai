name: Deploy to AWS

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**.md'
      - '.gitignore'
      - 'docs/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY_BACKEND: ${{ vars.ENVIRONMENT || 'production' }}/allthrive-backend
  ECS_CLUSTER: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-cluster
  ECS_SERVICE_WEB: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-web
  ECS_SERVICE_CELERY: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-celery
  ECS_SERVICE_BEAT: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-celery-beat
  S3_FRONTEND_BUCKET: allthrive-frontend-${{ vars.ENVIRONMENT || 'production' }}-${{ secrets.AWS_ACCOUNT_ID }}

permissions:
  id-token: write
  contents: read

jobs:
  # Note: Tests run on PR via ci.yml before merge
  # This workflow only deploys after merge to main

  # Validate Docker architecture before any deployment
  validate-architecture:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check Dockerfile.prod specifies correct platform
        run: |
          echo "üîç Checking Docker architecture configuration..."

          # Verify Dockerfile.prod exists
          if [ ! -f "Dockerfile.prod" ]; then
            echo "‚ùå Dockerfile.prod not found"
            exit 1
          fi

          # Check that the deploy workflow specifies linux/amd64
          if ! grep -q "platforms:.*linux/amd64" .github/workflows/deploy-aws.yml; then
            echo "‚ùå deploy-aws.yml should specify 'platforms: linux/amd64' for Docker builds"
            exit 1
          fi

          echo "‚úÖ Deploy workflow correctly specifies linux/amd64 platform"

      - name: Verify no ARM-only dependencies in Dockerfile.prod
        run: |
          echo "üîç Checking for architecture-specific issues..."

          # Check for common ARM-specific patterns that might cause issues
          if grep -E "arm64|aarch64" Dockerfile.prod | grep -v "#" | grep -v "AWS_CLI_ARCH"; then
            echo "‚ö†Ô∏è  Warning: Found ARM-specific references in Dockerfile.prod"
            echo "   Ensure these are handled correctly for x86_64 production"
          fi

          # Verify base image is available for amd64
          BASE_IMAGE=$(grep -m1 "^FROM" Dockerfile.prod | awk '{print $2}')
          echo "üì¶ Base image: $BASE_IMAGE"

          # python:3.11-slim is available for both architectures, so this should pass
          echo "‚úÖ Architecture check passed"

  # Validate migrations can be applied before deploying
  validate-migrations:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: allthrive_test
          POSTGRES_USER: allthrive
          POSTGRES_PASSWORD: allthrive
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U allthrive -d allthrive_test"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql://allthrive:allthrive@localhost:5432/allthrive_test
      SECRET_KEY: migration-validation-key
      DEBUG: "False"
      SECURE_SSL_REDIRECT: "False"
      ALLOWED_HOSTS: "localhost,127.0.0.1"
      CORS_ALLOWED_ORIGINS: "http://localhost:3000"
      COOKIE_DOMAIN: ""
      CSRF_TRUSTED_ORIGINS: "http://localhost:3000"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for Postgres
        run: |
          python - <<'PY'
          import os, time, psycopg2
          url = os.environ['DATABASE_URL']
          for i in range(30):
              try:
                  psycopg2.connect(url).close()
                  print('DB ready')
                  break
              except Exception as e:
                  print('Waiting for DB...', e)
                  time.sleep(1)
          else:
              raise SystemExit('Database not available')
          PY

      - name: Validate Django configuration
        run: |
          echo "üîç Checking Django configuration..."
          python manage.py check --deploy
          echo "‚úÖ Django configuration valid"

      - name: Validate migrations can be applied
        run: |
          echo "üîç Running migrations on fresh database..."
          python manage.py migrate --noinput
          echo "‚úÖ All migrations applied successfully"

      - name: Check for pending migrations
        run: |
          echo "üîç Checking for unmigrated model changes..."
          python manage.py makemigrations --check --dry-run
          echo "‚úÖ No pending migrations"

      - name: Show migration status
        run: |
          echo "üìã Migration status:"
          python manage.py showmigrations

  # Build and push backend Docker image
  build-backend:
    needs: [validate-architecture, validate-migrations]
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}
          tags: |
            type=sha,prefix=
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile.prod
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  # Build frontend and upload to S3
  build-frontend:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: frontend
        run: npm ci --legacy-peer-deps

      - name: Build frontend
        working-directory: frontend
        run: npm run build
        env:
          VITE_API_URL: ${{ vars.API_URL || 'https://api.allthrive.ai' }}
          VITE_WS_URL: ${{ vars.WS_URL || 'wss://api.allthrive.ai' }}
          VITE_APP_URL: ${{ vars.APP_URL || 'https://allthrive.ai' }}
          VITE_STRIPE_PUBLISHABLE_KEY: ${{ secrets.VITE_STRIPE_PUBLISHABLE_KEY }}
          VITE_POSTHOG_KEY: ${{ secrets.VITE_POSTHOG_KEY }}
          VITE_SENTRY_DSN: ${{ secrets.VITE_SENTRY_DSN }}
          VITE_RECAPTCHA_SITE_KEY: ${{ secrets.VITE_RECAPTCHA_SITE_KEY }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload to S3
        working-directory: frontend
        run: |
          aws s3 sync dist/ s3://${{ env.S3_FRONTEND_BUCKET }}/ \
            --delete \
            --cache-control "public, max-age=31536000" \
            --exclude "index.html" \
            --exclude "*.json"

          # Upload index.html and JSON files with no-cache
          aws s3 cp dist/index.html s3://${{ env.S3_FRONTEND_BUCKET }}/index.html \
            --cache-control "no-cache, no-store, must-revalidate"

          # Upload any JSON config files
          find dist -name "*.json" -exec aws s3 cp {} s3://${{ env.S3_FRONTEND_BUCKET }}/ \
            --cache-control "no-cache" \;

  # Deploy backend services to ECS
  deploy-backend:
    needs: build-backend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Update ECS Web Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_WEB }} \
            --force-new-deployment

      - name: Update ECS Celery Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_CELERY }} \
            --force-new-deployment

      - name: Update ECS Celery Beat Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_BEAT }} \
            --force-new-deployment

      - name: Wait for Web service stability
        run: |
          aws ecs wait services-stable \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE_WEB }}

  # Invalidate CloudFront cache
  deploy-frontend:
    needs: build-frontend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Get CloudFront Distribution ID
        id: cloudfront
        run: |
          DIST_ID=$(aws cloudformation describe-stacks \
            --stack-name ${{ vars.ENVIRONMENT || 'production' }}-allthrive-cloudfront \
            --query 'Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue' \
            --output text)
          echo "distribution_id=$DIST_ID" >> $GITHUB_OUTPUT

      - name: Invalidate CloudFront cache
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ steps.cloudfront.outputs.distribution_id }} \
            --paths "/*"

  # Run database migrations
  migrate:
    needs: deploy-backend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Get task ARN
        id: task
        run: |
          # Wait a moment for tasks to be fully running after deployment
          sleep 10

          TASK_ARN=$(aws ecs list-tasks \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service-name ${{ env.ECS_SERVICE_WEB }} \
            --desired-status RUNNING \
            --query 'taskArns[0]' \
            --output text)

          if [ -z "$TASK_ARN" ] || [ "$TASK_ARN" = "None" ]; then
            echo "‚ùå No running tasks found"
            exit 1
          fi

          echo "‚úÖ Found task: $TASK_ARN"
          echo "task_arn=$TASK_ARN" >> $GITHUB_OUTPUT

      - name: Install Session Manager plugin
        run: |
          curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb" -o "session-manager-plugin.deb"
          sudo dpkg -i session-manager-plugin.deb

      - name: Run migrations
        id: migrate
        run: |
          echo "üîÑ Running database migrations..."

          # Run migration with timeout and capture output
          # Using script to handle pseudo-terminal requirements
          MIGRATE_OUTPUT=$(timeout 300 aws ecs execute-command \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task ${{ steps.task.outputs.task_arn }} \
            --container web \
            --command "python manage.py migrate --noinput && echo 'MIGRATION_SUCCESS'" \
            --interactive 2>&1) || MIGRATE_EXIT=$?

          echo "$MIGRATE_OUTPUT"

          # Check if migration was successful
          if echo "$MIGRATE_OUTPUT" | grep -q "MIGRATION_SUCCESS"; then
            echo "‚úÖ Migrations completed successfully"
          elif echo "$MIGRATE_OUTPUT" | grep -q "No migrations to apply"; then
            echo "‚úÖ No migrations to apply"
          elif echo "$MIGRATE_OUTPUT" | grep -q "EOF" || echo "$MIGRATE_OUTPUT" | grep -q "Cannot perform start session"; then
            echo "‚ùå ECS Execute Command session failed - retrying..."
            sleep 5

            # Retry once
            MIGRATE_OUTPUT=$(timeout 300 aws ecs execute-command \
              --cluster ${{ env.ECS_CLUSTER }} \
              --task ${{ steps.task.outputs.task_arn }} \
              --container web \
              --command "python manage.py migrate --noinput && echo 'MIGRATION_SUCCESS'" \
              --interactive 2>&1) || true

            echo "$MIGRATE_OUTPUT"

            if echo "$MIGRATE_OUTPUT" | grep -q "MIGRATION_SUCCESS"; then
              echo "‚úÖ Migrations completed successfully on retry"
            elif echo "$MIGRATE_OUTPUT" | grep -q "No migrations to apply"; then
              echo "‚úÖ No migrations to apply"
            else
              echo "‚ùå Migration failed after retry"
              exit 1
            fi
          else
            echo "‚ùå Migration failed"
            exit 1
          fi

      - name: Setup OAuth providers
        run: |
          # Configure Django Site with correct domain (uses BACKEND_URL env var)
          timeout 60 aws ecs execute-command \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task ${{ steps.task.outputs.task_arn }} \
            --container web \
            --command "python manage.py setup_oauth" \
            --interactive || echo "‚ö†Ô∏è setup_oauth failed or timed out"

          # Setup Google OAuth (if credentials are configured)
          timeout 60 aws ecs execute-command \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task ${{ steps.task.outputs.task_arn }} \
            --container web \
            --command "python manage.py setup_google_oauth" \
            --interactive || echo "‚ö†Ô∏è setup_google_oauth skipped"

          # Setup GitHub OAuth (if credentials are configured)
          timeout 60 aws ecs execute-command \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task ${{ steps.task.outputs.task_arn }} \
            --container web \
            --command "python manage.py setup_github_oauth" \
            --interactive || echo "‚ö†Ô∏è setup_github_oauth skipped"

          # Setup LinkedIn OAuth (if credentials are configured)
          timeout 60 aws ecs execute-command \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task ${{ steps.task.outputs.task_arn }} \
            --container web \
            --command "python manage.py setup_linkedin_oauth" \
            --interactive || echo "‚ö†Ô∏è setup_linkedin_oauth skipped"

  # Notify on completion
  notify:
    needs: [deploy-backend, deploy-frontend, migrate]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send notification
        run: |
          if [ "${{ needs.deploy-backend.result }}" == "success" ] && \
             [ "${{ needs.deploy-frontend.result }}" == "success" ]; then
            echo "Deployment successful!"
          else
            echo "Deployment failed!"
            exit 1
          fi
