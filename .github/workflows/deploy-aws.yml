name: Deploy to AWS

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**.md'
      - '.gitignore'
      - 'docs/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY_BACKEND: ${{ vars.ENVIRONMENT || 'production' }}/allthrive-backend
  ECS_CLUSTER: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-cluster
  ECS_SERVICE_WEB: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-web
  ECS_SERVICE_CELERY: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-celery
  ECS_SERVICE_BEAT: ${{ vars.ENVIRONMENT || 'production' }}-allthrive-celery-beat
  S3_FRONTEND_BUCKET: allthrive-frontend-${{ vars.ENVIRONMENT || 'production' }}-${{ secrets.AWS_ACCOUNT_ID }}

permissions:
  id-token: write
  contents: read

jobs:
  # Note: Tests run on PR via ci.yml before merge
  # This workflow only deploys after merge to main

  # Validate Docker architecture before any deployment
  validate-architecture:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check Dockerfile.prod specifies correct platform
        run: |
          echo "üîç Checking Docker architecture configuration..."

          # Verify Dockerfile.prod exists
          if [ ! -f "Dockerfile.prod" ]; then
            echo "‚ùå Dockerfile.prod not found"
            exit 1
          fi

          # Check that the deploy workflow specifies linux/amd64
          if ! grep -q "platforms:.*linux/amd64" .github/workflows/deploy-aws.yml; then
            echo "‚ùå deploy-aws.yml should specify 'platforms: linux/amd64' for Docker builds"
            exit 1
          fi

          echo "‚úÖ Deploy workflow correctly specifies linux/amd64 platform"

      - name: Verify no ARM-only dependencies in Dockerfile.prod
        run: |
          echo "üîç Checking for architecture-specific issues..."

          # Check for common ARM-specific patterns that might cause issues
          if grep -E "arm64|aarch64" Dockerfile.prod | grep -v "#" | grep -v "AWS_CLI_ARCH"; then
            echo "‚ö†Ô∏è  Warning: Found ARM-specific references in Dockerfile.prod"
            echo "   Ensure these are handled correctly for x86_64 production"
          fi

          # Verify base image is available for amd64
          BASE_IMAGE=$(grep -m1 "^FROM" Dockerfile.prod | awk '{print $2}')
          echo "üì¶ Base image: $BASE_IMAGE"

          # python:3.11-slim is available for both architectures, so this should pass
          echo "‚úÖ Architecture check passed"

  # Validate migrations can be applied before deploying
  validate-migrations:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: allthrive_test
          POSTGRES_USER: allthrive
          POSTGRES_PASSWORD: allthrive
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U allthrive -d allthrive_test"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql://allthrive:allthrive@localhost:5432/allthrive_test
      SECRET_KEY: migration-validation-key
      DEBUG: "False"
      SECURE_SSL_REDIRECT: "False"
      ALLOWED_HOSTS: "localhost,127.0.0.1"
      CORS_ALLOWED_ORIGINS: "http://localhost:3000"
      COOKIE_DOMAIN: ""
      CSRF_TRUSTED_ORIGINS: "http://localhost:3000"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for Postgres
        run: |
          python - <<'PY'
          import os, time, psycopg2
          url = os.environ['DATABASE_URL']
          for i in range(30):
              try:
                  psycopg2.connect(url).close()
                  print('DB ready')
                  break
              except Exception as e:
                  print('Waiting for DB...', e)
                  time.sleep(1)
          else:
              raise SystemExit('Database not available')
          PY

      - name: Validate Django configuration
        run: |
          echo "üîç Checking Django configuration..."
          python manage.py check --deploy
          echo "‚úÖ Django configuration valid"

      - name: Validate migrations can be applied
        run: |
          echo "üîç Running migrations on fresh database..."
          python manage.py migrate --noinput
          echo "‚úÖ All migrations applied successfully"

      - name: Check for pending migrations
        run: |
          echo "üîç Checking for unmigrated model changes..."
          python manage.py makemigrations --check --dry-run
          echo "‚úÖ No pending migrations"

      - name: Show migration status
        run: |
          echo "üìã Migration status:"
          python manage.py showmigrations

  # Deploy CloudFormation infrastructure changes (if any)
  deploy-infrastructure:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Need previous commit to check for changes

      - name: Check for infrastructure changes
        id: check_changes
        run: |
          # Check if any CloudFormation files changed
          if git diff --name-only HEAD~1 HEAD | grep -q "infrastructure/cloudformation/"; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "üìã Infrastructure files changed:"
            git diff --name-only HEAD~1 HEAD | grep "infrastructure/cloudformation/" || true
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "‚úÖ No infrastructure changes detected"
          fi

      - name: Configure AWS credentials
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Get stack parameters
        if: steps.check_changes.outputs.has_changes == 'true'
        id: params
        run: |
          ENVIRONMENT="${{ vars.ENVIRONMENT || 'production' }}"
          ACCOUNT_ID="${{ secrets.AWS_ACCOUNT_ID }}"

          # Get existing parameters from CloudFormation stacks
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "account_id=$ACCOUNT_ID" >> $GITHUB_OUTPUT

      - name: Deploy ECS stack (if changed)
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          ENVIRONMENT="${{ steps.params.outputs.environment }}"
          STACK_NAME="${ENVIRONMENT}-allthrive-ecs"

          # Check if ECS template changed
          if git diff --name-only HEAD~1 HEAD | grep -q "10-ecs.yaml"; then
            echo "üöÄ Deploying ECS stack updates..."

            # Get current parameters from the existing stack
            CURRENT_PARAMS=$(aws cloudformation describe-stacks \
              --stack-name $STACK_NAME \
              --query 'Stacks[0].Parameters' \
              --output json 2>/dev/null || echo "[]")

            # Build parameter overrides to use existing values
            PARAM_OVERRIDES=""
            for key in $(echo $CURRENT_PARAMS | jq -r '.[].ParameterKey'); do
              PARAM_OVERRIDES="$PARAM_OVERRIDES ParameterKey=$key,UsePreviousValue=true"
            done

            aws cloudformation update-stack \
              --stack-name $STACK_NAME \
              --template-body file://infrastructure/cloudformation/10-ecs.yaml \
              --parameters $PARAM_OVERRIDES \
              --capabilities CAPABILITY_NAMED_IAM \
              --region ${{ env.AWS_REGION }} || {
                EXIT_CODE=$?
                if aws cloudformation describe-stacks --stack-name $STACK_NAME --query 'Stacks[0].StackStatus' --output text | grep -q "UPDATE_IN_PROGRESS\|CREATE_IN_PROGRESS"; then
                  echo "Stack update already in progress"
                elif [ $EXIT_CODE -eq 255 ]; then
                  echo "‚úÖ No updates needed for ECS stack"
                else
                  exit $EXIT_CODE
                fi
              }

            echo "‚è≥ Waiting for ECS stack update..."
            aws cloudformation wait stack-update-complete --stack-name $STACK_NAME --region ${{ env.AWS_REGION }} || true

            STATUS=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query 'Stacks[0].StackStatus' --output text)
            if [[ "$STATUS" == *"COMPLETE"* ]] && [[ "$STATUS" != *"ROLLBACK"* ]]; then
              echo "‚úÖ ECS stack updated successfully"
            else
              echo "‚ö†Ô∏è ECS stack status: $STATUS"
            fi
          else
            echo "‚úÖ No ECS stack changes"
          fi

      - name: Deploy CloudFront stack (if changed)
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          ENVIRONMENT="${{ steps.params.outputs.environment }}"
          STACK_NAME="${ENVIRONMENT}-allthrive-cloudfront"

          # Check if CloudFront template changed
          if git diff --name-only HEAD~1 HEAD | grep -q "11-cloudfront.yaml"; then
            echo "üöÄ Deploying CloudFront stack updates..."

            # Get current parameters from the existing stack
            CURRENT_PARAMS=$(aws cloudformation describe-stacks \
              --stack-name $STACK_NAME \
              --query 'Stacks[0].Parameters' \
              --output json 2>/dev/null || echo "[]")

            # Build parameter overrides to use existing values
            PARAM_OVERRIDES=""
            for key in $(echo $CURRENT_PARAMS | jq -r '.[].ParameterKey'); do
              PARAM_OVERRIDES="$PARAM_OVERRIDES ParameterKey=$key,UsePreviousValue=true"
            done

            aws cloudformation update-stack \
              --stack-name $STACK_NAME \
              --template-body file://infrastructure/cloudformation/11-cloudfront.yaml \
              --parameters $PARAM_OVERRIDES \
              --capabilities CAPABILITY_NAMED_IAM \
              --region ${{ env.AWS_REGION }} || {
                EXIT_CODE=$?
                if [ $EXIT_CODE -eq 255 ]; then
                  echo "‚úÖ No updates needed for CloudFront stack"
                else
                  exit $EXIT_CODE
                fi
              }

            echo "‚è≥ Waiting for CloudFront stack update (this can take several minutes)..."
            aws cloudformation wait stack-update-complete --stack-name $STACK_NAME --region ${{ env.AWS_REGION }} || true

            STATUS=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query 'Stacks[0].StackStatus' --output text)
            if [[ "$STATUS" == *"COMPLETE"* ]] && [[ "$STATUS" != *"ROLLBACK"* ]]; then
              echo "‚úÖ CloudFront stack updated successfully"
            else
              echo "‚ö†Ô∏è CloudFront stack status: $STATUS"
            fi
          else
            echo "‚úÖ No CloudFront stack changes"
          fi

      - name: Deploy ALB stack (if changed)
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          ENVIRONMENT="${{ steps.params.outputs.environment }}"
          STACK_NAME="${ENVIRONMENT}-allthrive-alb"

          # Check if ALB template changed
          if git diff --name-only HEAD~1 HEAD | grep -q "09-alb.yaml"; then
            echo "üöÄ Deploying ALB stack updates..."

            # Get current parameters from the existing stack
            CURRENT_PARAMS=$(aws cloudformation describe-stacks \
              --stack-name $STACK_NAME \
              --query 'Stacks[0].Parameters' \
              --output json 2>/dev/null || echo "[]")

            # Build parameter overrides to use existing values
            PARAM_OVERRIDES=""
            for key in $(echo $CURRENT_PARAMS | jq -r '.[].ParameterKey'); do
              PARAM_OVERRIDES="$PARAM_OVERRIDES ParameterKey=$key,UsePreviousValue=true"
            done

            aws cloudformation update-stack \
              --stack-name $STACK_NAME \
              --template-body file://infrastructure/cloudformation/09-alb.yaml \
              --parameters $PARAM_OVERRIDES \
              --capabilities CAPABILITY_NAMED_IAM \
              --region ${{ env.AWS_REGION }} || {
                EXIT_CODE=$?
                if [ $EXIT_CODE -eq 255 ]; then
                  echo "‚úÖ No updates needed for ALB stack"
                else
                  exit $EXIT_CODE
                fi
              }

            echo "‚è≥ Waiting for ALB stack update..."
            aws cloudformation wait stack-update-complete --stack-name $STACK_NAME --region ${{ env.AWS_REGION }} || true

            STATUS=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query 'Stacks[0].StackStatus' --output text)
            if [[ "$STATUS" == *"COMPLETE"* ]] && [[ "$STATUS" != *"ROLLBACK"* ]]; then
              echo "‚úÖ ALB stack updated successfully"
            else
              echo "‚ö†Ô∏è ALB stack status: $STATUS"
            fi
          else
            echo "‚úÖ No ALB stack changes"
          fi

  # Build and push backend Docker image
  build-backend:
    needs: [validate-architecture, validate-migrations, deploy-infrastructure]
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}
          tags: |
            type=sha,prefix=
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile.prod
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  # Build frontend and upload to S3
  build-frontend:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: frontend
        run: npm ci --legacy-peer-deps

      - name: Build frontend
        working-directory: frontend
        run: npm run build
        env:
          VITE_API_URL: ${{ vars.API_URL || 'https://api.allthrive.ai' }}
          VITE_WS_URL: ${{ vars.WS_URL || 'wss://api.allthrive.ai' }}
          VITE_APP_URL: ${{ vars.APP_URL || 'https://allthrive.ai' }}
          VITE_STRIPE_PUBLISHABLE_KEY: ${{ secrets.VITE_STRIPE_PUBLISHABLE_KEY }}
          VITE_POSTHOG_KEY: ${{ secrets.VITE_POSTHOG_KEY }}
          VITE_SENTRY_DSN: ${{ secrets.VITE_SENTRY_DSN }}
          VITE_RECAPTCHA_SITE_KEY: ${{ secrets.VITE_RECAPTCHA_SITE_KEY }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload to S3
        working-directory: frontend
        run: |
          aws s3 sync dist/ s3://${{ env.S3_FRONTEND_BUCKET }}/ \
            --delete \
            --cache-control "public, max-age=31536000" \
            --exclude "index.html" \
            --exclude "*.json"

          # Upload index.html and JSON files with no-cache
          aws s3 cp dist/index.html s3://${{ env.S3_FRONTEND_BUCKET }}/index.html \
            --cache-control "no-cache, no-store, must-revalidate"

          # Upload any JSON config files
          find dist -name "*.json" -exec aws s3 cp {} s3://${{ env.S3_FRONTEND_BUCKET }}/ \
            --cache-control "no-cache" \;

  # Deploy backend services to ECS
  deploy-backend:
    needs: build-backend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Update ECS Web Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_WEB }} \
            --force-new-deployment

      - name: Update ECS Celery Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_CELERY }} \
            --force-new-deployment

      - name: Update ECS Celery Beat Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE_BEAT }} \
            --force-new-deployment

      - name: Wait for Web service stability
        run: |
          aws ecs wait services-stable \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE_WEB }}

  # Invalidate CloudFront cache
  deploy-frontend:
    needs: build-frontend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Get CloudFront Distribution ID
        id: cloudfront
        run: |
          DIST_ID=$(aws cloudformation describe-stacks \
            --stack-name ${{ vars.ENVIRONMENT || 'production' }}-allthrive-cloudfront \
            --query 'Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue' \
            --output text)
          echo "distribution_id=$DIST_ID" >> $GITHUB_OUTPUT

      - name: Invalidate CloudFront cache
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ steps.cloudfront.outputs.distribution_id }} \
            --paths "/*"

  # Run database migrations using a one-off ECS task (more reliable than execute-command)
  migrate:
    needs: deploy-backend
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ vars.ENVIRONMENT || 'production' }}-allthrive-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Get service network configuration
        id: network
        run: |
          echo "üîç Getting network configuration from web service..."

          # Get the network configuration from the existing service
          NETWORK_CONFIG=$(aws ecs describe-services \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE_WEB }} \
            --query 'services[0].networkConfiguration.awsvpcConfiguration' \
            --output json)

          SUBNETS=$(echo $NETWORK_CONFIG | jq -r '.subnets | join(",")')
          SECURITY_GROUPS=$(echo $NETWORK_CONFIG | jq -r '.securityGroups | join(",")')
          ASSIGN_PUBLIC_IP=$(echo $NETWORK_CONFIG | jq -r '.assignPublicIp')

          echo "subnets=$SUBNETS" >> $GITHUB_OUTPUT
          echo "security_groups=$SECURITY_GROUPS" >> $GITHUB_OUTPUT
          echo "assign_public_ip=$ASSIGN_PUBLIC_IP" >> $GITHUB_OUTPUT

          # Get the task definition
          TASK_DEF=$(aws ecs describe-services \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE_WEB }} \
            --query 'services[0].taskDefinition' \
            --output text)

          echo "task_definition=$TASK_DEF" >> $GITHUB_OUTPUT
          echo "‚úÖ Network config retrieved"

      - name: Run migrations
        id: migrate
        run: |
          echo "üîÑ Running database migrations via one-off task..."

          # Run a one-off task with migration command override
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task-definition ${{ steps.network.outputs.task_definition }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ steps.network.outputs.subnets }}],securityGroups=[${{ steps.network.outputs.security_groups }}],assignPublicIp=${{ steps.network.outputs.assign_public_ip }}}" \
            --overrides '{"containerOverrides":[{"name":"web","command":["python","manage.py","migrate","--noinput"]}]}' \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "üì¶ Started migration task: $TASK_ARN"
          echo "task_arn=$TASK_ARN" >> $GITHUB_OUTPUT

          # Wait for the task to complete
          echo "‚è≥ Waiting for migration task to complete..."
          aws ecs wait tasks-stopped \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN

          # Check the exit code
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[?name==`web`].exitCode' \
            --output text)

          if [ "$EXIT_CODE" = "0" ]; then
            echo "‚úÖ Migrations completed successfully"
          else
            echo "‚ùå Migration failed with exit code: $EXIT_CODE"

            # Get the stopped reason for debugging
            STOPPED_REASON=$(aws ecs describe-tasks \
              --cluster ${{ env.ECS_CLUSTER }} \
              --tasks $TASK_ARN \
              --query 'tasks[0].stoppedReason' \
              --output text)
            echo "Stopped reason: $STOPPED_REASON"

            exit 1
          fi

      - name: Run post-deploy commands
        run: |
          echo "üîß Running post-deployment commands..."

          # Run OAuth setup as a one-off task
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task-definition ${{ steps.network.outputs.task_definition }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ steps.network.outputs.subnets }}],securityGroups=[${{ steps.network.outputs.security_groups }}],assignPublicIp=${{ steps.network.outputs.assign_public_ip }}}" \
            --overrides '{"containerOverrides":[{"name":"web","command":["sh","-c","python manage.py setup_oauth && python manage.py setup_google_oauth || true && python manage.py setup_github_oauth || true && python manage.py setup_linkedin_oauth || true"]}]}' \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "üì¶ Started post-deploy task: $TASK_ARN"

          # Wait for completion
          aws ecs wait tasks-stopped \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN

          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[?name==`web`].exitCode' \
            --output text)

          if [ "$EXIT_CODE" = "0" ]; then
            echo "‚úÖ Post-deploy commands completed"
          else
            echo "‚ö†Ô∏è Post-deploy commands had issues (exit code: $EXIT_CODE) - continuing anyway"
          fi

      - name: Seed data (idempotent - creates only if not exists)
        run: |
          echo "üå± Running seed commands (idempotent - will update or create)..."

          # Run seed commands as a one-off task
          # These use update_or_create so they're safe to run on every deploy
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task-definition ${{ steps.network.outputs.task_definition }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ steps.network.outputs.subnets }}],securityGroups=[${{ steps.network.outputs.security_groups }}],assignPublicIp=${{ steps.network.outputs.assign_public_ip }}}" \
            --overrides '{"containerOverrides":[{"name":"web","command":["sh","-c","python manage.py seed_topics && python manage.py seed_taxonomies && python manage.py seed_categories && python manage.py seed_tools && python manage.py seed_challenge_types && python manage.py seed_billing && python manage.py seed_achievements && python manage.py create_pip"]}]}' \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "üì¶ Started seed task: $TASK_ARN"

          # Wait for completion
          aws ecs wait tasks-stopped \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN

          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[?name==`web`].exitCode' \
            --output text)

          if [ "$EXIT_CODE" = "0" ]; then
            echo "‚úÖ Seed commands completed successfully"
          else
            echo "‚ö†Ô∏è Seed commands had issues (exit code: $EXIT_CODE) - continuing anyway"
            # Don't fail the deploy for seed issues - data may already exist
          fi

  # Notify on completion
  notify:
    needs: [deploy-backend, deploy-frontend, migrate]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send notification
        run: |
          if [ "${{ needs.deploy-backend.result }}" == "success" ] && \
             [ "${{ needs.deploy-frontend.result }}" == "success" ]; then
            echo "Deployment successful!"
          else
            echo "Deployment failed!"
            exit 1
          fi
